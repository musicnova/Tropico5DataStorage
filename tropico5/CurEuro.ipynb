{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install quandl\n",
    "H={'/home/user/HADOOP': 'C:\\HADOOP',\n",
    "  '/home/user/HADOOP/spark-2.3.0-bin-hadoop2.7': 'C:\\HADOOP\\spark-2.3.0-bin-hadoop2.7',\n",
    "  '/home/user/Program Files/java/8.0.172-zulu': 'C:\\Program Files\\Java\\jdk1.8.0_172',\n",
    "  '/home/user/TMP/SPARK': 'C:\\TMP\\SPARK'}\n",
    "#H=dict([(k,k) for k in H.keys()])\n",
    "Z=\"ok\"\n",
    "SB=\"SANDBOX.USER\"\n",
    "DB=\"MY_DATA\"\n",
    "TB=\"CURRENCY_KAFKA\"\n",
    "BA=\"SANDBOX.USER\"\n",
    "AB=\"REQUEST_CURRENCY\"\n",
    "CB=\"SANDBOX.USER\"\n",
    "BC=\"RESPONSE_CURRENCY\"\n",
    "DIR=\"C:/ALBINA/SASHA/SOAP\"\n",
    "C=[(\"errcode\",\"BIGINT\",\"cast(errcode as bigint)\")   \n",
    "  ,(\"prefix\",\"\",\"\")\n",
    "  ,(\"itemnum\",\"BIGINT\",\"cast(itemnum as bigint)\")\n",
    "  ,(\"item\",\"\",\"\")\n",
    "  ,(\"suffix\",\"\",\"\")\n",
    "  ,(\"remaining\",\"BIGINT\",\"cast(remaining as bigint)\")\n",
    "  ,(\"nextkey\",\"\",\"\")\n",
    "]\n",
    "D=[(\"errcode\",\"BIGINT\",\"cast(errcode as bigint)\")\n",
    "  ,(\"prefix\",\"\",\"\")\n",
    "  ,(\"itemnum\",\"BIGINT\",\"cast(itemnum as bigint)\")\n",
    "  ,(\"item\",\"\",\"\")\n",
    "  ,(\"suffix\",\"\",\"\")\n",
    "  ,(\"remaining\",\"BIGINT\",\"cast(remaining as bigint)\")\n",
    "  ,(\"nextkey\",\"\",\"\")\n",
    "]\n",
    "AKEY=\"mqZagQhmMKp1w1-565-E\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "def parse_html(fname, nrows=-1):\n",
    "    import pandas as pd\n",
    "    fn = fname.replace(\".html\", \".csv\")\n",
    "    return pd.read_csv(fn)\n",
    "\n",
    "def parse_html(fname, nrows=-1):\n",
    "    import pandas as pd\n",
    "    # /html/body/section[1]/div/div/div/div[1454]/div[1]: 928\n",
    "    # /html/body/section[1]/div/div/div/div[1454]/div[2]: 0350000-0449999\n",
    "    # /html/body/section[1]/div/div/div/div[1454]/div[3]: Краснодар  Краснодарский край\n",
    "    # /html/body/section[1]/div/div/div/div[1454]/div[4]: ОАО \"Мегафон\" Кавказский филиал\n",
    "    # /html/body/section[1]/div/div/div/div[1454]/div[4]/p: 29.03.2010-01.01.2038\n",
    "    d = pd.DataFrame(columns=['prefix', 'fromto', 'area', 'vendor', 'dates'])\n",
    "    import lxml.html as LH\n",
    "    import lxml.etree as LE\n",
    "    import codecs\n",
    "    import re\n",
    "    a=[]\n",
    "    m={}\n",
    "    with codecs.open(DIR+\"/\"+\"/defcodes.html\", \"r\", 'utf-8') as rf:\n",
    "        text = rf.read()\n",
    "        root = LH.fromstring(text)\n",
    "        tree = LE.ElementTree(root)\n",
    "        s = set([])\n",
    "        for e in root.iter():\n",
    "            t = tree.getpath(e)\n",
    "            if t.startswith(\"/html/body/section[1]/div/div/div/div[\"):\n",
    "                s.add(t)\n",
    "        a = list(sorted(s))\n",
    "        m = dict([(re.sub(\"/div.*$\", \"\"\n",
    "                , x.replace(\"/html/body/section[1]/div/div/div/div[\", \"\", 1).replace(\"]\", \"\", 1)), 1) for x in a])\n",
    "\n",
    "        for k in m.keys():\n",
    "            row = []\n",
    "            if len(d) >= nrows and nrows >= 0: break\n",
    "            s = \"\"\n",
    "            for v in tree.xpath(\"/html/body/section[1]/div/div/div/div[\"+k+\"]/div[1]\" + '/text()'):\n",
    "                s += v.lstrip().rstrip()\n",
    "            row.append(s)\n",
    "            s = \"\"\n",
    "            for v in tree.xpath(\"/html/body/section[1]/div/div/div/div[\"+k+\"]/div[2]\" + '/text()'):\n",
    "                s += v.lstrip().rstrip()\n",
    "            row.append(s)\n",
    "            s = \"\"\n",
    "            for v in tree.xpath(\"/html/body/section[1]/div/div/div/div[\"+k+\"]/div[3]\" + '/text()'):\n",
    "                s += v.lstrip().rstrip()\n",
    "            row.append(s)\n",
    "            s = \"\"\n",
    "            for v in tree.xpath(\"/html/body/section[1]/div/div/div/div[\"+k+\"]/div[4]\" + '/text()'):\n",
    "                s += v.lstrip().rstrip()\n",
    "            row.append(s)\n",
    "            s = \"\"\n",
    "            for v in tree.xpath(\"/html/body/section[1]/div/div/div/div[\"+k+\"]/div[4]/p\" + '/text()'):\n",
    "                s += v.lstrip().rstrip()\n",
    "            row.append(s)\n",
    "            d.loc[len(d)+1] = row\n",
    "    return d\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "view = parse_html0(DIR+\"/\"+\"/defcodes.html\", nrows=100)\n",
    "display(view[view.index < 5])\n",
    "\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "os.environ['HADOOP_HOME'] = 'C:\\HADOOP'\n",
    "os.environ['SPARK_HOME'] = 'C:\\HADOOP\\spark-2.3.0-bin-hadoop2.7'\n",
    "os.environ['JAVA_HOME'] = 'C:\\Program Files\\Java\\jdk1.8.0_172'\n",
    "\n",
    "def start():\n",
    "    import time\n",
    "    from pyspark.sql import SparkSession\n",
    "    res = SparkSession.builder.master('local[*]').appName('test') \\\n",
    "       .config('spark.sql.warehouse.dir', 'file:///' + WARE) \\\n",
    "       .config(\"spark.driver.maxResultSize\", \"600g\") \\\n",
    "       .config(\"spark.sql.shuffle.output.partitions\", \"32768\") \\\n",
    "       .getOrCreate()\n",
    "    time.sleep(1)\n",
    "    return res\n",
    "\n",
    "def stop(sc):\n",
    "    print(\"ok\")\n",
    "    sc.stop()\n",
    "\n",
    "def preview(df):\n",
    "    df.show(n=3, truncate=False)\n",
    "\n",
    "WARE=\"C:\\TMP\\SPARK\"\n",
    "os.makedirs(WARE, exist_ok=True)\n",
    "os.chmod(WARE, 0o777)\n",
    "spark=start()\n",
    "print(spark.version)\n",
    "\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def spark_table(name, sqlc):\n",
    "    import pyspark.storagelevel as L\n",
    "    # README https://stackoverflow.com/a/22742982\n",
    "    # README https://stackoverflow.com/a/40866286\n",
    "    NO_OF_EXECUTOR_INSTANCES = int(dict(sqlc.sparkContext._conf.getAll()).get(\"spark.executor.instances\",5))\n",
    "    NO_OF_EXECUTOR_CORES = int(dict(sqlc.sparkContext._conf.getAll()).get(\"spark.executor.cores\",2))\n",
    "    REPARTITION_FACTOR = 10\n",
    "    ideal = NO_OF_EXECUTOR_INSTANCES * NO_OF_EXECUTOR_CORES * REPARTITION_FACTOR\n",
    "\n",
    "    import shutil\n",
    "    import pandas as pd\n",
    "    if name == SB+'_'+TB:\n",
    "        shutil.copyfile(DIR+\"/\"+\"manyphones.tsv\", WARE+\"/ig363_\"+name+\".csv\")\n",
    "    else:\n",
    "        shutil.rmtree(WARE+\"/ig363_\"+name+\".csv\")\n",
    "    q = [\"cast(_c\"+str(i)+\" as string) as \" + C[i][0] for i in range(len(C))]\n",
    "    qq = [(C[i][2] if len(C[i][2]) else C[i][0])+' as '+C[i][0] for i in range(len(C))]\n",
    "    return sqlc.read.option(\"escape\", \"\\\"\").option(\"quote\", \"\\\"\").option(\"sep\", \"\\t\").csv(WARE+\"/ig363_\"+name+\".csv\",\n",
    "                header=False).persist(L.StorageLevel.MEMORY_AND_DISK).repartition(1).selectExpr(q).selectExpr(qq)\n",
    "\n",
    "def spark_table2(name, sqlc):\n",
    "    import pyspark.storagelevel as L\n",
    "\n",
    "    import shutil\n",
    "    import pandas as pd\n",
    "    if name == SB+'_'+BC:\n",
    "        d = parse_html0(DIR+\"/\"+\"defcodes.html\")\n",
    "        d.to_csv(DIR+\"/\"+\"defcodes.csv\", header=None, index=None)\n",
    "        shutil.copyfile(DIR+\"/\"+\"defcodes.csv\", WARE+\"/ig363_\"+name+\".csv\")\n",
    "    else:\n",
    "        shutil.rmtree(WARE+\"/ig363_\"+name+\".csv\")\n",
    "    q = [\"cast(_c\"+str(i)+\" as string) as \" + D[i][0] for i in range(len(D))]\n",
    "    return sqlc.read.option(\"escape\", \"\\\"\").option(\"quote\", \"\\\"\").option(\"sep\", \",\").csv(WARE+\"/ig363_\"+name+\".csv\",\n",
    "                header=False).persist(L.StorageLevel.MEMORY_AND_DISK).repartition(1).selectExpr(q)\n",
    "\n",
    "def write_insert(df, dst):\n",
    "    df.write.option(\"escape\", \"\\\"\").option(\"quote\", \"\\\"\").csv(WARE+\"/ig363_\"+dst, header=False, mode=\"Overwrite\")\n",
    "\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.window as W\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "def nfunc(x):\n",
    "    try:\n",
    "        yyyy = '{0:04d}'.format(int(x[0:4])) if len(x) >= 4 else \"1970\"\n",
    "        mm = '{0:02d}'.format(int(x[5:7])) if len(x) >= 7 else \"01\"\n",
    "        dd = '{0:02d}'.format(int(x[8:10])) if len(x) >= 10 else \"01\"\n",
    "        return yyyy + \"-\" + mm + \"-\" + dd\n",
    "    except ValueError:\n",
    "        return '1970-01-01' \n",
    "\n",
    "def get_norm_udf():\n",
    "    import pyspark.sql.functions as F\n",
    "    import pyspark.sql.types as T\n",
    "    return F.udf(lambda x: nfunc(x), T.StringType())\n",
    "\n",
    "df = spark_table(SB+\"_\"+TB, spark)\n",
    "df = df.fillna({'lastdt':'1970-01-01'})\n",
    "df = df.withColumn(\"normdt\", get_norm_udf()(F.col(\"lastdt\"))).drop(\"lastdt\")\n",
    "df = df.withColumn(\"lastdt\", F.col(\"normdt\")).drop(\"normdt\")\n",
    "df = df.selectExpr(\"phone\", \"lastdt\", \"lower(source) as source\")\n",
    "write_insert(df.coalesce(1), SB+\".\"+TB)\n",
    "preview(df.filter(F.col(\"source\") == \"velobike\"))\n",
    "\n",
    "dd = spark_table2(SB+\"_\"+BC, spark)\n",
    "write_insert(dd.coalesce(1), SB+\".\"+BC)\n",
    "preview(dd)\n",
    "\n",
    "dd = dd.withColumn(\"a\", F.concat(F.lit(\"7\"), F.col(\"prefix\"), F.split(F.col(\"fromto\"), \"-\")[0]))\n",
    "dd = dd.withColumn(\"b\", F.concat(F.lit(\"7\"), F.col(\"prefix\"), F.split(F.col(\"fromto\"), \"-\")[1]))\n",
    "dd = dd.withColumn(\"hash\", F.concat(F.lit(\"7\"), F.col(\"prefix\")))\n",
    "dd = dd.selectExpr(\"cast(a as bigint) as a\", \"cast(b as bigint) as b\", \"cast(hash as bigint) as hash\", \"area\")\n",
    "preview(dd)\n",
    "\n",
    "dr = df.selectExpr(\"cast(phone as bigint) as phone\", \"lastdt\", \"source\")\n",
    "dr = dr.withColumn(\"is_2018\", F.when(F.col(\"lastdt\") >= F.lit(\"2018-01-01\"), F.lit(1)).otherwise(F.lit(0)))\n",
    "dr = dr.drop(\"lastdt\")\n",
    "dr = dr.groupBy(F.col(\"phone\")).agg(F.max(F.col(\"is_2018\")).alias(\"is_2018\"),F.count(\"source\").alias(\"cnt\"))\n",
    "dr = dr.withColumn(\"key\", F.substring(F.col(\"phone\"), 0, 4))\n",
    "dr = dr.selectExpr(\"cast(key as bigint) as key\", \"phone\", \"is_2018\", \"cnt\")\n",
    "dr = dr.selectExpr(\"phone\", \"is_2018\", \"cnt\", \"key\")\n",
    "write_insert(dr.coalesce(1), SB+\".\"+CD)\n",
    "preview(dr)\n",
    "\n",
    "#res = dr.selectExpr(\"cast(key as bigint) as key\", \"phone\", \"is_2018\", \"cnt\")\n",
    "#res = res.join(dd, res[\"key\"] == dd[\"hash\"], how=\"left_outer\")\n",
    "#res = res.filter(F.col(\"phone\") >= F.col(\"a\"))\n",
    "#res = res.filter(F.col(\"phone\") <= F.col(\"b\"))\n",
    "#res = res.drop(\"a\").drop(\"b\").drop(\"key\").drop(\"hash\")\n",
    "#res = res.withColumn(\"is_msc\", F.when(F.col(\"area\") == F.lit(\"Москва Москва\"), F.lit(1)).otherwise(F.lit(0)))\n",
    "#res = res.withColumn(\"is_mo\", F.when(F.col(\"area\") == F.lit(\"Москва Москва\"), F.lit(1)).otherwise(F.lit(0)))\n",
    "#res = res.selectExpr(\"phone\", \"is_2018\", \"cnt\", \"area\", \"is_msc\", \"is_mo\")\n",
    "#write_insert(res.coalesce(1), SB+\".\"+CD)\n",
    "#preview(res)\n",
    "\n",
    "\n",
    "\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# https://docs.quandl.com/\n",
    "# https://blog.quandl.com/api-for-currency-data\n",
    "\n",
    "import requests\n",
    "answer1 = requests.get('https://www.quandl.com/api/v3/datasets/BOE/XUDLBK69/data.json?api_key='+AKEY, stream=True)\n",
    "res1 = answer1.text\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(res1)\n",
    "\n",
    "answer2 = requests.get('https://www.quandl.com/api/v3/datasets/ECD/EURRUB/data.json?api_key='+AKEY, stream=True)\n",
    "res2 = answer2.text\n",
    "\n",
    "pprint.pprint(res2)\n",
    "\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a=[]\n",
    "a.append((\"load_tags\", \"\", \"\"))\n",
    "a.append((\"load_rank\", \"BIGINT\", \"cast(load_rank as bigint)\"))\n",
    "b=[]\n",
    "b.append((\"a\", \"BIGINT\", \"cast(concat('7', prefix, split(fromto, '-')[0]) as bigint)\"))\n",
    "b.append((\"b\", \"BIGINT\", \"cast(concat('7', prefix, split(fromto, '-')[1]) as bigint)\"))\n",
    "b.append((\"hash\", \"BIGINT\", \"cast(concat('7', prefix) as bigint)\"))\n",
    "b.append((\"load_tags\", \"\", \"concat('')\"))\n",
    "b.append((\"load_rank\", \"BIGINT\", \"row_number() over()\"))\n",
    "c=[]\n",
    "c.append((\"load_tags\", \"\", \"\"))\n",
    "c.append((\"load_rank\", \"BIGINT\", \"cast(load_rank as bigint)\"))\n",
    "print(\"(1) rename \"+WARE+\"/ig363_\"+SB+\".\"+TB)\n",
    "print(\"part.csv to\")\n",
    "print(\"many.csv\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(2) upload to ambari (permissions 444)\")\n",
    "print(\"make folder\")\n",
    "print(\"/user/yurbasov/tmp/phones_many\")\n",
    "print(\"permissions 777\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(3) CREATE EXTERNAL TABLE \"+SB+\"_\"+TB+\"(\")\n",
    "print('\\n,'.join(['`'+C[i][0]+'`  string' for i in range(len(C))]))\n",
    "print(\")\")\n",
    "print(\"ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\")\n",
    "print(\"WITH SERDEPROPERTIES (\")\n",
    "print(\"\\\"separatorChar\\\" = \\\",\\\",\")\n",
    "print(\"\\\"quoteChar\\\"     = \\\"\\\\\\\"\\\"\")\n",
    "print(\") STORED as textfile LOCATION '/user/yurbasov/tmp/phones_many/'\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(4) CREATE TABLE \"+SB+\"_\"+AB+\"(\")\n",
    "print('\\n,'.join(['`'+C[i][0]+'`  ' + (C[i][1] if len(C[i][1]) else 'string') for i in range(len(C))]))\n",
    "print(\",\")\n",
    "print('\\n,'.join(['`'+a[i][0]+'`  ' + (a[i][1] if len(a[i][1]) else 'string') for i in range(len(a))]))\n",
    "print(\") STORED AS ORC tblproperties (\\\"orc.compress\\\"=\\\"SNAPPY\\\")\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(5) INSERT INTO \"+SB+\"_\"+AB)\n",
    "print(\"SELECT\")\n",
    "print('\\n,'.join(['case when `'+C[i][0]+'`=\\'\\' then null else '+(C[i][2] if len(C[i][2]) else C[i][0])+' end as '+C[i][0] for i in range(len(C))]))\n",
    "print(',\"\" as load_tags')\n",
    "print(',row_number() over() as load_rank')\n",
    "print(\" FROM \"+SB+\"_\"+TB)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(6) rename \"+WARE+\"/ig363_\"+SB+\".\"+BC)\n",
    "print(\"part.csv to\")\n",
    "print(\"dict.csv\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(7) upload to ambari (permissions 444)\")\n",
    "print(\"make folder\")\n",
    "print(\"/user/yurbasov/tmp/dicts_many\")\n",
    "print(\"permissions 777\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(8) CREATE EXTERNAL TABLE \"+SB+\"_\"+BC+\"(\")\n",
    "print('\\n,'.join(['`'+D[i][0]+'`  string' for i in range(len(D))]))\n",
    "print(\")\")\n",
    "print(\"ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\")\n",
    "print(\"WITH SERDEPROPERTIES (\")\n",
    "print(\"\\\"separatorChar\\\" = \\\",\\\",\")\n",
    "print(\"\\\"quoteChar\\\"     = \\\"\\\\\\\"\\\"\")\n",
    "print(\") STORED as textfile LOCATION '/user/yurbasov/tmp/dicts_many/'\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(9) CREATE TABLE \"+CB+\".\"+BC+\"(\")\n",
    "print('\\n,'.join(['`'+D[i][0]+'`  ' + (D[i][1] if len(D[i][1]) else 'string') for i in range(len(D))]))\n",
    "print(\",\")\n",
    "print('\\n,'.join(['`'+b[i][0]+'`  ' + (b[i][1] if len(b[i][1]) else 'string') for i in range(len(b))]))\n",
    "print(\") STORED AS ORC tblproperties (\\\"orc.compress\\\"=\\\"SNAPPY\\\")\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(10) INSERT INTO \"+CB+\".\"+BC)\n",
    "print(\"SELECT\")\n",
    "print('\\n,'.join(['case when `'+D[i][0]+'`=\\'\\' then null else '+(D[i][2] if len(D[i][2]) else D[i][0])+' end as '+D[i][0] for i in range(len(D))]))\n",
    "print(\",\")\n",
    "print('\\n,'.join([''+(b[i][2] if len(b[i][2]) else b[i][0])+' as '+b[i][0] for i in range(len(b))]))\n",
    "print(\" FROM \"+SB+\"_\"+BC)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(11) rename \"+WARE+\"/ig363_\"+SB+\".\"+CD)\n",
    "print(\"part.csv to\")\n",
    "print(\"result.csv\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(12) upload to ambari (permissions 444)\")\n",
    "print(\"make folder\")\n",
    "print(\"/user/yurbasov/tmp/results_many\")\n",
    "print(\"permissions 777\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(13) CREATE EXTERNAL TABLE \"+SB+\"_\"+CD+\"(\")\n",
    "print('\\n,'.join(['`'+E[i][0]+'`  string' for i in range(len(E))]))\n",
    "print(\")\")\n",
    "print(\"ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\")\n",
    "print(\"WITH SERDEPROPERTIES (\")\n",
    "print(\"\\\"separatorChar\\\" = \\\",\\\",\")\n",
    "print(\"\\\"quoteChar\\\"     = \\\"\\\\\\\"\\\"\")\n",
    "print(\") STORED as textfile LOCATION '/user/yurbasov/tmp/results_many/'\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(14) CREATE TABLE \"+SB+\"_\"+CD+\"_SNAPPY(\")\n",
    "print('\\n,'.join(['`'+E[i][0]+'`  ' + (E[i][1] if len(E[i][1]) else 'string') for i in range(len(E))]))\n",
    "print(\",\")\n",
    "print('\\n,'.join(['`'+c[i][0]+'`  ' + (c[i][1] if len(c[i][1]) else 'string') for i in range(len(c))]))\n",
    "print(\") STORED AS ORC tblproperties (\\\"orc.compress\\\"=\\\"SNAPPY\\\")\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(15) INSERT INTO \"+SB+\"_\"+CD+\"_SNAPPY\")\n",
    "print(\"SELECT\")\n",
    "print('\\n,'.join(['case when `'+E[i][0]+'`=\\'\\' then null else '+(E[i][2] if len(E[i][2]) else E[i][0])+' end as '+E[i][0] for i in range(len(E))]))\n",
    "print(',\"\" as load_tags')\n",
    "print(',row_number() over() as load_rank')\n",
    "print(\" FROM \"+SB+\"_\"+CD)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(16*) CREATE TABLE \"+SB+\"_\"+CD+\"_RESULT\")\n",
    "print(\"AS\")\n",
    "print(\"WITH pre AS\")\n",
    "print(\"(SELECT a, b, area,\")\n",
    "print('\\n,'.join(['`'+E[i][0]+'`' for i in range(len(E))]))\n",
    "print(\",cast(phone as bigint) as tel\")\n",
    "print(\" FROM \"+SB+\"_\"+CD+\"_SNAPPY s\")\n",
    "print(\"LEFT JOIN\")\n",
    "print(CB+\".\"+BC+\" d\")\n",
    "print(\"ON d.hash = s.key\")\n",
    "print(\")\")\n",
    "print(\" SELECT tel, is_2018, cnt, area, case when area = 'Москва Москва' then 1 else 0 end as is_msc FROM pre\")\n",
    "print(\"WHERE tel >= a\")\n",
    "print(\"AND  tel <= b\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(17*) SELECT * FROM \"+SB+\"_\"+CD+\"_RESULT\"+\" WHERE load_tags = ''\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(18*) in ambari download to hdfs\")\n",
    "print(\"/user/yurbasov/tmp/yurbasov_many_results\"+\".csv.gz\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"(19*) from ambari download from hdfs\")\n",
    "print(DIR+\"/yurbasov_many_results\"+\".csv.gz\")\n",
    "print(\"unpack to \"+DB+\".\"+CD+\" (.csv)\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(Z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
