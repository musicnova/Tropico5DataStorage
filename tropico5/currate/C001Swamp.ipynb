{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "Z='ok'\n",
    "D={\"CURRENCIES\": \"1\", \"USDRUB\": \"2\", \"EURRUB\": \"3\"}\n",
    "A=\"a8a53f5f50807538e3b7dd4483bc8cac\"\n",
    "\n",
    "import time\n",
    "NOW=int(time.strftime(\"%Y%m%d\"))\n",
    "WORKDIR=\"C:\\\\code\\\\currate\\\\loadpart=\"+str(NOW)\n",
    "RUNDIR=\"C:\\\\Users\\\\Home\\\\CODE_GITLAB\\\\tropico5\\\\currate\"\n",
    "\n",
    "# print(RUNDIR)\n",
    "# TASK: PARSE WEBPAGE http://tropico.wikia.com/wiki/Buildings_(Tropico_5)\n",
    "\n",
    "# USING: https://github.com/niwinz/phantompy (FROM https://toster.ru/q/171125)\n",
    "# READ: http://phantompy.readthedocs.io/en/latest/api-python.html\n",
    "\n",
    "# GOOGLE: site:github.com phantompy -niwinz\n",
    "# EXAMPLE: https://github.com/c3h3/phantompy/blob/master/phantompy/crawler.py\n",
    "# DEPRICATED: https://docs.python.org/2/library/commands.html\n",
    "\n",
    "#!pip install phantompy\n",
    "#!pip install htmldom\n",
    "#!pip install bitstring\n",
    "\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress={}\n",
    "\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res_commands =  447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, subprocess\n",
    "import uuid\n",
    "\n",
    "def crawl(url, run_dir, work_dir, main_dir, raw_subdir, d_nums, filename=None):\n",
    "    os.makedirs(os.path.join(work_dir, main_dir, raw_subdir), exist_ok=True)\n",
    "\n",
    "    if filename==None:\n",
    "        filename = uuid.uuid4().hex\n",
    "    \n",
    "    filepath = os.path.join(work_dir, main_dir, raw_subdir, filename)\n",
    "    current_dir = run_dir\n",
    "    \n",
    "    # USING: https://docs.python.org/3/library/urllib.request.html#module-urllib.request\n",
    "    import bitstring\n",
    "    import codecs as cd\n",
    "    import requests as rq\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:45.0) Gecko/20100101 Firefox/45.0'\n",
    "      }\n",
    "    with rq.get(url, headers=headers, verify=False) as b:\n",
    "        with cd.open(filepath, \"w\", \"utf-8\") as wf:\n",
    "            buff = bitstring.BitStream()\n",
    "            for chunk in b.iter_content(1024):\n",
    "                buff.append(chunk)\n",
    "            res = buff.bytes.decode('utf-8')\n",
    "            res_commands = str(len(res))\n",
    "            wf.write(res)    \n",
    "    \n",
    "    print(\"res_commands = \", res_commands)\n",
    "\n",
    "    os.chdir(work_dir)\n",
    "    os.chdir(main_dir)\n",
    "    os.chdir(raw_subdir)\n",
    "    with open(filename, \"r\") as rf:\n",
    "        res_text = rf.read()\n",
    "    os.chdir(current_dir)\n",
    "    \n",
    "    res_dict = {}\n",
    "    res_dict[\"res_commands\"] = res_commands\n",
    "    res_dict[\"res_text\"] = res_text  \n",
    "    res_dict[\"filepath\"] = filepath \n",
    "    res_dict[\"filename\"] = filename\n",
    "    \n",
    "    return res_dict \n",
    "\n",
    "name_01 = D[\"CURRENCIES\"] + \".txt\"\n",
    "progress[\"01crawl:currate_ru:currencies\"] = crawl(\"https://currate.ru/api/?get=currency_list&key=\" + A,\n",
    "         RUNDIR, WORKDIR, os.path.join(\"data=swamp\", \"source=currate_ru\", \"table=currencies\"), \"step=01crawl\", D, name_01)\n",
    "\n",
    "#print(Z)\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res_commands =  81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, subprocess\n",
    "import uuid\n",
    "\n",
    "name_02 = D[\"USDRUB\"] + \".txt\"\n",
    "progress[\"02crawl:currate_ru:usdrub\"] = crawl(\" https://currate.ru/api/?get=rates&pairs=USDRUB,RUBUSD&key=\" + A,\n",
    "         RUNDIR, WORKDIR, os.path.join(\"data=swamp\", \"source=currate_ru\", \"table=usdrub\"), \"step=01crawl\", D, name_02)\n",
    "\n",
    "#print(Z)\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res_commands =  83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, subprocess\n",
    "import uuid\n",
    "\n",
    "name_03 = D[\"EURRUB\"] + \".txt\"\n",
    "progress[\"03crawl:currate_ru:eurrub\"] = crawl(\" https://currate.ru/api/?get=rates&pairs=EURRUB,RUBEUR&key=\" + A,\n",
    "         RUNDIR, WORKDIR, os.path.join(\"data=swamp\", \"source=currate_ru\", \"table=eurrub\"), \"step=01crawl\", D, name_03)\n",
    "\n",
    "#print(Z)\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'lxml' has no attribute 'html'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-f56ce1a0ed24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m progress[\"02parse:currate_ru:currencies\"] = parse(progress[\"01crawl:currate_ru:currencies\"][\"filepath\"],\n\u001b[0;32m     58\u001b[0m             \u001b[0mRUNDIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWORKDIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data=swamp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"source=currate_ru\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"table=buildings\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \"step=02parse\", D, name_02)\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-f56ce1a0ed24>\u001b[0m in \u001b[0;36mparse\u001b[1;34m(html_path, run_dir, work_dir, main_dir, csv_subdir, d_nums, filename)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlxml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlxml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0metree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mElementTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'lxml' has no attribute 'html'"
     ]
    }
   ],
   "source": [
    "import os, subprocess\n",
    "\n",
    "def parse(html_path, run_dir, work_dir, main_dir, csv_subdir, d_nums, filename=None):\n",
    "    os.makedirs(os.path.join(work_dir, main_dir, csv_subdir), exist_ok=True)\n",
    "\n",
    "    if filename==None:\n",
    "        filename = uuid.uuid4().hex\n",
    "\n",
    "    filepath = os.path.join(work_dir, main_dir, csv_subdir, filename)\n",
    "    current_dir = run_dir\n",
    "        \n",
    "    # USING: https://habrahabr.ru/post/280238/   https://stackoverflow.com/a/1577495\n",
    "    import lxml\n",
    "\n",
    "    with open(html_path, \"r\") as rf:\n",
    "        text = rf.read()\n",
    "\n",
    "    root = lxml.html.fromstring(text)\n",
    "    tree = lxml.etree.ElementTree(root)\n",
    "    s = set([])\n",
    "    for e in root.iter():\n",
    "        s.add(tree.getpath(e))\n",
    "    a = list(sorted(s))\n",
    "    \n",
    "    # USING: https://docs.python.org/2/library/csv.html\n",
    "    import csv\n",
    "    import codecs as cd\n",
    "    with cd.open(filepath, \"w\", \"utf-8\") as wf:\n",
    "        spamwriter = csv.writer(wf, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        row = []\n",
    "        for i in range(0, len(a)):\n",
    "            s = \"\"\n",
    "            for v in tree.xpath(a[i] + '/text()'):\n",
    "                s += v\n",
    "            row.append(s)\n",
    "        #print(str(row))\n",
    "        spamwriter.writerow(row)\n",
    "        res_commands = str(len(a))\n",
    "\n",
    "    os.chdir(work_dir)\n",
    "    os.chdir(main_dir)\n",
    "    os.chdir(csv_subdir)\n",
    "    with open(filename, \"r\") as rf:\n",
    "        res_text = rf.read()\n",
    "    os.chdir(current_dir)\n",
    "\n",
    "    res_dict = {}\n",
    "    res_dict[\"res_commands\"] = res_commands\n",
    "    res_dict[\"res_text\"] = res_text  \n",
    "    res_dict[\"filepath\"] = filepath \n",
    "    res_dict[\"filename\"] = filename\n",
    "    \n",
    "    return res_dict\n",
    "\n",
    "name_02 = name_01\n",
    "\n",
    "progress[\"02parse:currate_ru:currencies\"] = parse(progress[\"01crawl:currate_ru:currencies\"][\"filepath\"],\n",
    "            RUNDIR, WORKDIR, os.path.join(\"data=swamp\", \"source=currate_ru\", \"table=buildings\"),\n",
    "            \"step=02parse\", D, name_02)\n",
    "\n",
    "Z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
